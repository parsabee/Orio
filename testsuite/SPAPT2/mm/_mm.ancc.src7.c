__global__ void orcu_kernel34190(const int m, const int n, int p, double* A, double* B, double* C) {
  const int tid=blockIdx.x*blockDim.x+threadIdx.x;
  const int gsize=gridDim.x*blockDim.x;
  int j, k;
  for (int i=tid; i<=m-1; i+=gsize) {
    for (j=0; j<=n-1; j++ ) {
      for (k=0; k<=p-1; k++ ) {
        C[i*n+j]=C[i*n+j]+A[i*p+k]*B[k*n+j];
      }
    }
  }
}
void MatMatMult(double* A, double* B, double* C, int m, int n, int p) {

/*@ begin PerfTuning (
    def performance_params {
        param thread_count[]  = [32, 64];
        param block_count[]  = range(14,28,14);
        param inner_loop_unroll_fact[] = range(1, 5);
        param preferred_L1_cache[]  = [16, 48];
        param stream_count[] = [1, 2, 4, 8];
        param CFLAGS[] = ['-O3'];
    }

    def input_params
    {
        param CONT = 500;
        param NCONT = 500;
        param M = 500;
    }

    def input_vars
    {
        decl static double A[M * M] = random;
        decl static double B[M * M] = random;
        decl static double C[M * M] = 0;
    }

    def search
    {
        arg algorithm = 'Randomlocal';
        arg total_runs = 1000;
    }

    def build {
      arg build_command = 'nvcc -arch=sm_75 @CFLAGS';
    }

    def performance_counter {
      arg method = 'basic timer';
      arg repetitions = 1;
    }
) @*/
/**-- (Generated by Orio) 
Best performance cost: 
  [50.1045] 
Tuned for specific problem sizes: 
  CONT = 500 
  M = 500 
  NCONT = 500 
Best performance parameters: 
  CFLAGS = -O3 
  block_count = 14 
  inner_loop_unroll_fact = 1 
  preferred_L1_cache = 48 
  stream_count = 8 
  thread_count = 32 
--**/


int m = M, p = M, n = M;

#define max(x,y)    ((x) > (y)? (x) : (y))
#define min(x,y)    ((x) < (y)? (x) : (y))

/*@ begin Loop(
  transform CUDA(threadCount=thread_count,
                 blockCount=block_count,
                 preferL1Size=preferred_L1_cache,
                 unrollInner=inner_loop_unroll_fact,
                 streamCount=stream_count)
  for(i=0; i<=m-1; i++)
    for(j=0; j<=n-1; j++) {
      for(k=0; k<=p-1; k++){
        C[i*n+j] += A[i*p+k]*B[k*n+j];
      }
    }
) @*/
{
  cudaDeviceSynchronize();
  /*declare variables*/
  double* dev_A;
  double* dev_B;
  double* dev_C;
  int nthreads=32;
  int nstreams=8;
  /*calculate device dimensions with the cuda calculator*/
  dim3 dimGrid, dimBlock;
  dimBlock.x=nthreads;
  dimGrid.x=14;
  /*create streams*/
  int istream, soffset;
  cudaStream_t stream[nstreams+1];
  for (istream=0; istream<=nstreams; istream++ ) 
    cudaStreamCreate(&stream[istream]);
  int chunklen=m/nstreams;
  int chunkrem=m%nstreams;
  /*allocate device memory*/
  cudaMalloc(&dev_A,M *M*sizeof(double));
  cudaHostRegister(A,M *M*sizeof(double),cudaHostRegisterPortable);
  cudaMalloc(&dev_B,M *M*sizeof(double));
  cudaHostRegister(B,M *M*sizeof(double),cudaHostRegisterPortable);
  cudaMalloc(&dev_C,M *M*sizeof(double));
  cudaHostRegister(C,M *M*sizeof(double),cudaHostRegisterPortable);
  cudaDeviceSetCacheConfig(cudaFuncCachePreferL1);
  /*copy data from host to device*/
  cudaEventRecord(tstart,0);
  for (istream=0; istream<nstreams; istream++ ) {
    soffset=istream*chunklen;
    cudaMemcpyAsync(dev_A+soffset,A+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
    cudaMemcpyAsync(dev_B+soffset,B+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
    cudaMemcpyAsync(dev_C+soffset,C+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
  }
  if (chunkrem!=0) {
    soffset=istream*chunklen;
    cudaMemcpyAsync(dev_A+soffset,A+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
    cudaMemcpyAsync(dev_B+soffset,B+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
    cudaMemcpyAsync(dev_C+soffset,C+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
  }
  cudaEventRecord(tstop,0);
  cudaEventSynchronize(tstop);
  cudaEventElapsedTime(&orcu_transfer,tstart,tstop);
  cudaEventRecord(start,0);
  /*invoke device kernel*/
  int blks4chunk=dimGrid.x/nstreams;
  if (dimGrid.x%nstreams!=0) 
    blks4chunk++ ;
  for (istream=0; istream<nstreams; istream++ ) {
    soffset=istream*chunklen;
    orcu_kernel34190<<<blks4chunk,dimBlock,0,stream[istream]>>>(chunklen,n,p,dev_A+soffset,dev_B+soffset,dev_C+soffset);
  }
  if (chunkrem!=0) {
    soffset=istream*chunklen;
    orcu_kernel34190<<<blks4chunk,dimBlock,0,stream[istream]>>>(chunkrem,n,p,dev_A+soffset,dev_B+soffset,dev_C+soffset);
  }
  cudaEventRecord(stop,0);
  cudaEventSynchronize(stop);
  cudaEventElapsedTime(&orcu_elapsed,start,stop);
  /*copy data from device to host*/
  for (istream=0; istream<nstreams; istream++ ) {
    soffset=istream*chunklen;
    cudaMemcpyAsync(C+soffset,dev_C+soffset,chunklen*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
  }
  if (chunkrem!=0) {
    soffset=istream*chunklen;
    cudaMemcpyAsync(C+soffset,dev_C+soffset,chunkrem*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
  }
  for (istream=0; istream<=nstreams; istream++ ) 
    cudaStreamSynchronize(stream[istream]);
  cudaDeviceSetCacheConfig(cudaFuncCachePreferNone);
  for (istream=0; istream<=nstreams; istream++ ) 
    cudaStreamDestroy(stream[istream]);
  /*free allocated memory*/
  cudaFree(dev_A);
  cudaFree(dev_B);
  cudaFree(dev_C);
  cudaHostUnregister(A);
  cudaHostUnregister(B);
  cudaHostUnregister(C);
  cudaError_t err=cudaGetLastError();
  if (cudaSuccess!=err) 
    printf("CUDA runtime error: %s@",cudaGetErrorString(err));
}
/*@ end @*/
/*@ end @*/


}



